{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with class imbalance:\n",
    "- resampling techniques\n",
    "    - under or over sampling random vs informed\n",
    "    - SMOTE synthetic minor ...\n",
    "- kappa statistics/ MCC Metric\n",
    "- multiclass mcc \"comparing two k-category assignments by a k-category correlation coeeficient\"\n",
    "\n",
    "\n",
    "spatial pyramid pooling in deep convolutional networks for visual recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from pathlib import Path\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import Sequence\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.util import pad\n",
    "from skimage.util import crop\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_Gen(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels, batch_size, shuffle):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_labels = len(np.unique(labels))\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "    \n",
    "    def crop_or_pad(self, image, dim):\n",
    "        x, y, _ = image.shape\n",
    "        if y < dim and x < dim:\n",
    "            image = pad(image, ((math.ceil((dim - image.shape[0])/2),math.floor((dim - image.shape[0])/2)),\n",
    "                                (math.ceil((dim - image.shape[1])/2),math.floor((dim - image.shape[1])/2)), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "        elif y > dim and x > dim:\n",
    "            rand1 = random.randint(1,x-dim)\n",
    "            rand2 = random.randint(1,y-dim)\n",
    "            image = crop(image, ((rand1,x-dim-rand1),(rand2,y-dim-rand2),(0,0)))\n",
    "        elif x > dim:\n",
    "            rand1 = random.randint(1,x-dim)\n",
    "            image = pad(image, ((0,0),\n",
    "                                (math.ceil((dim - y)/2),math.floor((dim - y)/2)), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "            image = crop(image, ((rand1,x-dim-rand1),\n",
    "                                (0,0), (0,0)))\n",
    "        else:\n",
    "            rand2 = random.randint(1,y-dim)\n",
    "            image = pad(image, ((math.ceil((dim - x)/2),math.floor((dim - x)/2)),\n",
    "                                (0,0), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "            image = crop(image, ((0,0),\n",
    "                                 (rand2,y-dim-rand2), (0,0)))\n",
    "        return image\n",
    "    \n",
    "    def read_im(self, filename, dim):\n",
    "        image = imread(filename)\n",
    "        #image = resize(image, (dim,dim), anti_aliasing = True, mode = \"reflect\")\n",
    "        image = skimage.color.gray2rgb(image)\n",
    "        image = self.crop_or_pad(image, dim)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        image = [self.read_im(filename, 299) for filename in batch_x]\n",
    "        image = (image-np.amin(image))/(np.amax(image)-np.amin(image))\n",
    "        batch_y = keras.utils.to_categorical(batch_y, self.num_labels)\n",
    "        return np.array(image), np.array(batch_y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            fnames_and_labels = list(zip(self.image_filenames, self.labels))\n",
    "            random.shuffle(fnames_and_labels)\n",
    "            self.image_filenames, self.labels = zip(*fnames_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = './data/test/'\n",
    "def fetch_data_set(path, ftype = 'jpg'):\n",
    "    p = Path(path)\n",
    "    files = list(p.glob('**/*.'+ftype))\n",
    "    classes = str(files).split('/')\n",
    "    classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "    classnames, indices = np.unique(classes, return_inverse=True)\n",
    "    dict_classes = dict(zip(classnames, list(range(0,len(classes)))))\n",
    "    return files, classes, dict_classes\n",
    "\n",
    "#np.array([dict_[i] for i in classes])\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from itertools import chain\n",
    "\n",
    "def over_under_sample(files, classes, dict_classes, num_to_undersample = 15000, num_to_oversample = 7500):\n",
    "    files_array = np.array(files).reshape(-1,1)\n",
    "    dic = {}\n",
    "    for i in list(classes):\n",
    "        dic[i] = dic.get(i,0) + 1\n",
    "\n",
    "    classes_to_oversample = dict((k, v) for k, v in dic.items() if v < num_to_oversample)\n",
    "    classes_to_undersample = dict((k, v) for k, v in dic.items() if v > num_to_undersample)    \n",
    "    \n",
    "    for key, value in classes_to_undersample.items():\n",
    "        classes_to_undersample[key] = num_to_undersample\n",
    "    for key, value in classes_to_oversample.items():\n",
    "        classes_to_oversample[key] = num_to_oversample\n",
    "    \n",
    "    ros = RandomOverSampler(sampling_strategy = classes_to_oversample)\n",
    "    rus = RandomUnderSampler(sampling_strategy = classes_to_undersample)\n",
    "    x_over, y_over = ros.fit_resample(files_array, classes)\n",
    "    x_under, y_under = rus.fit_resample(x_over, y_over)\n",
    "    x_under = list(chain(*x_under.tolist()))\n",
    "    return x_under, y_under\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_train_validation_test(data_X, data_Y, test_percent, validation_percent, seed): \n",
    "    assert (test_percent < 1) and (0 < validation_percent) and (validation_percent < 1)\n",
    "    X_tmp, X_val, Y_tmp, Y_val = train_test_split(data_X, data_Y, test_size=validation_percent, shuffle=True, random_state=seed)\n",
    "    \n",
    "    if test_percent != 0:\n",
    "        relative_test_percent = test_percent / (1 - validation_percent)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_tmp, Y_tmp, test_size=relative_test_percent, shuffle=True, random_state=seed)\n",
    "        split_data = [X_train, Y_train, X_val, Y_val, X_test, Y_test]\n",
    "    else:\n",
    "        X_train, Y_train = X_tmp, Y_tmp\n",
    "        split_data = [X_train, Y_train, X_val, Y_val]\n",
    "\n",
    "    return split_data  \n",
    "\n",
    "def encode_labels(labels, OneHot=True, encoder = None):\n",
    "    if OneHot and encoder == None:\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        enc.fit(np.array(labels).reshape(-1, 1))\n",
    "        OneHot = enc.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
    "        return OneHot, enc\n",
    "    else:\n",
    "        OneHot = encoder.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
    "        return OneHot\n",
    "    # mlb \n",
    "    #1 convert labels to multilabel using hardcoded dict\n",
    "    #2 fit mlb\n",
    "    #3 transform \n",
    "    # return labels and encoder\n",
    "    # repeat for when encoder is present\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "files, classes, dict_classes = fetch_data_set(\"./data/imgs/\")\n",
    "# split dataset\n",
    "split = split_data_train_validation_test(files, classes, 0.05, .25, random.randint(1,10000))\n",
    "split[0], split[1] = over_under_sample(split[0], split[1], dict_classes)\n",
    "# load label encoder and transform labels\n",
    "split[1], OH_enc = encode_labels(split[1], OneHot = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from collections import Counter\n",
    "#print(sorted(Counter(classes).items()))\n",
    "#print(sorted(Counter(y_re).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annelida': 0,\n",
       " 'Bivalvia__Mollusca': 1,\n",
       " 'Brachyura': 2,\n",
       " 'Candaciidae': 3,\n",
       " 'Cavoliniidae': 4,\n",
       " 'Centropagidae': 5,\n",
       " 'Corycaeidae': 6,\n",
       " 'Coscinodiscus': 7,\n",
       " 'Decapoda': 8,\n",
       " 'Doliolida': 9,\n",
       " 'Eucalanidae': 10,\n",
       " 'Euchaetidae': 11,\n",
       " 'Evadne': 12,\n",
       " 'Foraminifera': 13,\n",
       " 'Fritillariidae': 14,\n",
       " 'Haloptilus': 15,\n",
       " 'Harpacticoida': 16,\n",
       " 'Limacinidae': 17,\n",
       " 'Noctiluca': 18,\n",
       " 'Oikopleuridae': 19,\n",
       " 'Oncaeidae': 20,\n",
       " 'Ostracoda': 21,\n",
       " 'Penilia': 22,\n",
       " 'Phaeodaria': 23,\n",
       " 'Salpida': 24,\n",
       " 'Temoridae': 25,\n",
       " 'calyptopsis': 26,\n",
       " 'cyphonaute': 27,\n",
       " 'egg__Actinopterygii': 28,\n",
       " 'egg__other': 29,\n",
       " 'eudoxie__Diphyidae': 30,\n",
       " 'gonophore__Diphyidae': 31,\n",
       " 'multiple__Copepoda': 32,\n",
       " 'multiple__other': 33,\n",
       " 'nauplii__Cirripedia': 34,\n",
       " 'nauplii__Crustacea': 35,\n",
       " 'nectophore__Diphyidae': 36,\n",
       " 'tail__Appendicularia': 37,\n",
       " 'tail__Chaetognatha': 38,\n",
       " 'zoea__Decapoda': 39}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c,_= np.unique(split[5], return_inverse=True)\n",
    "#len(c)\n",
    "#b = {}\n",
    "#for i in list(split[5]):\n",
    "#    b[i] = b.get(i,0) + 1\n",
    "\n",
    "#counts = dict((k, v) for k, v in b.items() if v > 1)\n",
    "#counts\n",
    "\n",
    "#f, c, d = fetch_data_set('./data/imgs')\n",
    "#d['cavo'] = [0,1,1]\n",
    "#[d[i] for i in c]\n",
    "d_ = d\n",
    "d_['Annelida'] = ['Annelida', 'Metazoa', 'Eukaryota']\n",
    "d_['Bivalvia__Molusca'] = ['Bivalvia__Molusca', 'Mollusca', 'Metazoa', 'Eukaryota']\n",
    "d_['Brachyura'] = ['Brachyura', 'Decapoda', 'Malacostraca', 'Arthropoda', 'Metazoa', 'Eukaryota']\n",
    "d_['Candaciidae'] = ['Candaciidae', 'Calanoida', 'Copepoda', 'Maxillopoda', 'Arthropoda', 'Metazoa', 'Eukaryota']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./data/test/') \n",
    "files = list(p.glob('**/*.jpg'))\n",
    "classes = str(files).split('/')\n",
    "classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "classnames, indices = np.unique(classes, return_inverse=True)\n",
    "labels = keras.utils.to_categorical(indices, len(np.unique(indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./data/imgs/') \n",
    "files = list(p.glob('**/*.jpg'))\n",
    "classes = str(files).split('/')\n",
    "classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "classnames, indices = np.unique(classes, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for i in list(classes):\n",
    "    b[i] = b.get(i,0) + 1\n",
    "\n",
    "counts = dict((k, v) for k, v in b.items() if v > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([15000, 15000, 15000, 15000])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(list(counts.values()))\n",
    "counts\n",
    "classes_to_oversample = dict((k, v) for k, v in b.items() if v < 7500)\n",
    "classes_to_undersample = dict((k, v) for k, v in b.items() if v > 15000)\n",
    "#assign new target values for sampling\n",
    "for key, value in classes_to_undersample.items():\n",
    "    classes_to_undersample[key] = 15000\n",
    "classes_to_undersample.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_train_validation_test(data_X, data_Y, test_percent, validation_percent, seed): \n",
    "    assert (test_percent < 1) and (0 < validation_percent) and (validation_percent < 1)\n",
    "    X_tmp, X_val, Y_tmp, Y_val = train_test_split(data_X, data_Y, test_size=validation_percent, shuffle=True, random_state=seed)\n",
    "    \n",
    "    if test_percent != 0:\n",
    "        relative_test_percent = test_percent / (1 - validation_percent)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_tmp, Y_tmp, test_size=relative_test_percent, shuffle=True, random_state=seed)\n",
    "        split_data = [X_train, Y_train, X_val, Y_val, X_test, Y_test]\n",
    "    else:\n",
    "        X_train, Y_train = X_tmp, Y_tmp\n",
    "        split_data = [X_train, Y_train, X_val, Y_val]\n",
    "\n",
    "    return split_data\n",
    "\n",
    "split = split_data_train_validation_test(files, classes, 0, .25, 1190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 75), (1, 78), (2, 72)]\n",
      "[(0, 150), (1, 300), (2, 72)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florianmuthreich/.local/lib/python3.5/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (150) in class 0 will be larger than the number of samples in the majority class (class #1 -> 78)\n",
      "  n_samples_majority))\n",
      "/home/florianmuthreich/.local/lib/python3.5/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (300) in class 1 will be larger than the number of samples in the majority class (class #1 -> 78)\n",
      "  n_samples_majority))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "d = {0: 150, 1: 300}\n",
    "ros = RandomOverSampler(random_state = 0, sampling_strategy = d)\n",
    "x_re, y_re = ros.fit_resample(np.array(split[0]).reshape(-1,1), split[1])\n",
    "from collections import Counter\n",
    "print(sorted(Counter(split[1]).items()))\n",
    "print(sorted(Counter(y_re).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 2, 2, 2, 0, 0, 0, 1, 2, 0,\n",
       "       1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 2, 0, 2, 2, 1, 0, 2,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 2, 1, 1, 1,\n",
       "       2, 0, 0, 0, 0, 1, 2, 0, 2, 1, 1, 2, 2, 1, 0, 2, 2, 1, 2, 0, 1, 0,\n",
       "       0, 2, 1, 2, 1, 1, 0, 1, 2, 1, 2, 0, 0, 1, 2, 0, 0, 2, 2, 1, 0, 1,\n",
       "       0, 2, 2, 2, 0, 2, 1, 1, 2, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 1,\n",
       "       1, 2, 2, 0, 0, 1, 0, 2, 1, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0,\n",
       "       2, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1,\n",
       "       1, 2, 1, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 1, 2, 1, 1, 1, 0, 1,\n",
       "       1, 2, 1, 0, 1, 2, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2,\n",
       "       2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "x_re = list(chain(*x_re.tolist()))\n",
    "y_re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a512a4a96f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "#import keras_metrics\n",
    "#metrics=[keras_metrics.precision(), keras_metrics.recall()\n",
    "b = {}\n",
    "for i in list(np.array(x_re)):\n",
    "    b[i] = b.get(i,0) + 1\n",
    "\n",
    "dict((k, v) for k, v in b.items() if v > 1)\n",
    "\n",
    "#x_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import keras_metrics\n",
    "from keras import backend as K\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "#num_training_samples=len(files)\n",
    "\n",
    "files, classes, dict_classes =  fetch_data_set('./data/test')\n",
    "split = split_data_train_validation_test(files, classes, .1, .25, 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-d662273c92b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#len(np.argmax(model_pred, axis = 1, out = None)), len(test_pred), len(split[5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#[dict_classes[i] for i in split[5]], list(test_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdict_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'get_key'"
     ]
    }
   ],
   "source": [
    "pd.crosstab(np.array([dict_classes[i] for i in split[5]]), (test_pred), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "#len(split[5]), len(test_pred)\n",
    "#len(np.argmax(model_pred, axis = 1, out = None)), len(test_pred), len(split[5])\n",
    "#[dict_classes[i] for i in split[5]], list(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "arrays and names must have the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-43ecb337adba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#test_actu = np.argmax([dict_classes[i] for i in split[5]], axis = 1, out = None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mcrosstab\u001b[0;34m(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_make_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0mrownames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'col'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36m_get_names\u001b[0;34m(arrs, names, prefix)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays and names must have the same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: arrays and names must have the same length"
     ]
    }
   ],
   "source": [
    "#[dict_classes[i] for i in split[1]]\n",
    "import pandas as pd\n",
    "model_pred = model.predict_generator(generator = test_batch, steps = (len(split[4]) // batch_size))\n",
    "#test_actu = np.argmax([dict_classes[i] for i in split[5]], axis = 1, out = None)\n",
    "test_pred = np.argmax(model_pred, axis = 1, out = None)\n",
    "pd.crosstab(split[5], test_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 3s 475ms/step - loss: 0.0510 - precision: 0.9815 - recall: 0.9464 - acc: 0.9781 - val_loss: 0.3522 - val_precision: 1.0000 - val_recall: 0.8947 - val_acc: 0.8500\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.0385 - precision: 0.9815 - recall: 1.0000 - acc: 0.9890 - val_loss: 1.2495 - val_precision: 1.0000 - val_recall: 0.4545 - val_acc: 0.7111\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.0330 - precision: 1.0000 - recall: 0.9259 - acc: 0.9869 - val_loss: 0.3546 - val_precision: 1.0000 - val_recall: 0.7143 - val_acc: 0.8593\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.0482 - precision: 0.9444 - recall: 0.9623 - acc: 0.9815 - val_loss: 1.0048 - val_precision: 1.0000 - val_recall: 0.5294 - val_acc: 0.7556\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.0794 - precision: 0.9000 - recall: 0.9643 - acc: 0.9625 - val_loss: 0.2189 - val_precision: 1.0000 - val_recall: 0.8667 - val_acc: 0.9185\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.0377 - precision: 1.0000 - recall: 0.9138 - acc: 0.9831 - val_loss: 1.8346 - val_precision: 1.0000 - val_recall: 0.5000 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.0408 - precision: 0.9818 - recall: 1.0000 - acc: 0.9869 - val_loss: 0.5376 - val_precision: 1.0000 - val_recall: 0.7857 - val_acc: 0.8278\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 1s 244ms/step - loss: 0.0337 - precision: 0.9649 - recall: 0.9821 - acc: 0.9909 - val_loss: 0.6455 - val_precision: 1.0000 - val_recall: 0.4118 - val_acc: 0.8444\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 0.0143 - precision: 1.0000 - recall: 0.9600 - acc: 0.9944 - val_loss: 0.8780 - val_precision: 1.0000 - val_recall: 0.9231 - val_acc: 0.7926\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.0181 - precision: 1.0000 - recall: 1.0000 - acc: 0.9944 - val_loss: 0.5733 - val_precision: 1.0000 - val_recall: 0.7895 - val_acc: 0.8389\n"
     ]
    }
   ],
   "source": [
    "training_batch = MY_Gen(split[0], [dict_classes[i] for i in split[1]], batch_size, shuffle = True)\n",
    "validation_batch = MY_Gen(split[2], [dict_classes[i] for i in split[3]], batch_size, shuffle = True)#files, indices\n",
    "test_batch = MY_Gen(split[4], [dict_classes[i] for i in split[5]], batch_size, shuffle = False)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras_metrics.precision(), keras_metrics.recall(),'accuracy'])\n",
    "\n",
    "history = model.fit_generator(generator=training_batch,\n",
    "                    validation_data = validation_batch,\n",
    "                    validation_steps = (len(split[2]) // batch_size),\n",
    "                    steps_per_epoch=(len(split[0]) // batch_size),\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=16,\n",
    "                    max_queue_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "with open(\"data/output/models/pretra_INC.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "model.save_weights(\"data/output/models/pretra_INC_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im=np.asarray(Image.open(files[1]).resize([299,299]))\n",
    "#im = im/np.amax(im)\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(image[7])\n",
    "#plt.show()\n",
    "#ind = np.arange(105)\n",
    "#isinstance(classes, list)\n",
    "#len(classes)\n",
    "import random\n",
    "c=list(zip(files,classes))\n",
    "random.shuffle(c)\n",
    "files,classes = zip (*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "im = resize(imread(files[1]), (100, 100))\n",
    "im = skimage.color.gray2rgb(im)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im = np.array([(Image.open(file_name).resize([299,299])) for file_name in files])\n",
    "#im2 = np.array([\n",
    "           # resize(imread(file_name), (299, 299))\n",
    "            #   for file_name in files])\n",
    "#len(im2)\n",
    "#im2.shape\n",
    "#im.shape\n",
    "def norm_im(filename, dim):\n",
    "    image = imread(filename)\n",
    "    image = resize(image, (dim,dim), mode = \"edge\")\n",
    "    image = (image-np.amin(image))/(np.amax(image)-np.amin(image))\n",
    "    return image\n",
    "image = np.array([norm_im(filename, 100) for filename in files])\n",
    "\n",
    "\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files[range(1,10)]\n",
    "list(indices)\n",
    "keras.utils.to_categorical(indices, 5)\n",
    "len(np.unique(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "num_training_samples=len(files)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "my_training_batch_generator = MY_Gen(files, labels, batch_size)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(generator=my_training_batch_generator,\n",
    "                                          steps_per_epoch=(num_training_samples // batch_size),\n",
    "                                          epochs=10,\n",
    "                                          verbose=1,\n",
    "                                          use_multiprocessing=False,\n",
    "                                          workers=16,\n",
    "                                          max_queue_size=32,\n",
    "                             shuffle = True)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path('./data/imgs/') \n",
    "classes = [x for x in p.iterdir() if x.is_dir()]\n",
    "files = list(p.glob('**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 0, 0, 1, 0, 0]]),\n",
       " [('blue', 'dress', 'no_legs'), ('black', 'blue', 'dress', 'no_legs')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels = [\n",
    "    (\"legs\",\"blue\", \"jeans\"),\n",
    "    (\"no_legs\",\"blue\", \"dress\"),\n",
    "    (\"no_legs\",\"red\", \"dress\"),\n",
    "    (\"no_legs\",\"red\", \"shirt\"),\n",
    "    (\"no_legs\",\"blue\", \"shirt\"),\n",
    "    (\"no_legs\",\"black\", \"jeans\")]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "\n",
    "mlb.classes_\n",
    "\n",
    "mlb.transform([(\"blue\", \"dress\", \"no_legs\")]), mlb.inverse_transform(np.array([[0, 1, 1, 0, 0, 1, 0, 0], [1, 1, 1, 0, 0, 1, 0, 0]]))\n",
    "#np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.array(split[5]).reshape(-1, 1))\n",
    "OneHot=enc.transform(np.array(split[5]).reshape(-1, 1)).toarray()\n",
    "#enc.inverse_transform(OneHot)\n",
    "OneHot[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/test/cavo/992612.jpg'),\n",
       " PosixPath('data/test/cavo/1158831.jpg'),\n",
       " PosixPath('data/test/cavo/993180.jpg'),\n",
       " PosixPath('data/test/cavo/996279.jpg'),\n",
       " PosixPath('data/test/cavo/12747088.jpg'),\n",
       " PosixPath('data/test/cavo/1158620.jpg'),\n",
       " PosixPath('data/test/cavo/992773.jpg'),\n",
       " PosixPath('data/test/cavo/994128.jpg'),\n",
       " PosixPath('data/test/cavo/1158767.jpg'),\n",
       " PosixPath('data/test/cavo/996858.jpg'),\n",
       " PosixPath('data/test/cavo/993866.jpg'),\n",
       " PosixPath('data/test/cavo/993300.jpg'),\n",
       " PosixPath('data/test/cavo/12746529.jpg'),\n",
       " PosixPath('data/test/cavo/12746695.jpg'),\n",
       " PosixPath('data/test/cavo/12746864.jpg'),\n",
       " PosixPath('data/test/cavo/1158573.jpg'),\n",
       " PosixPath('data/test/cavo/12747169.jpg'),\n",
       " PosixPath('data/test/cavo/994685.jpg'),\n",
       " PosixPath('data/test/cavo/993520.jpg'),\n",
       " PosixPath('data/test/cavo/995387.jpg'),\n",
       " PosixPath('data/test/cavo/993928.jpg'),\n",
       " PosixPath('data/test/cavo/1158518.jpg'),\n",
       " PosixPath('data/test/cavo/12747201.jpg'),\n",
       " PosixPath('data/test/cavo/993824.jpg'),\n",
       " PosixPath('data/test/cavo/993086.jpg'),\n",
       " PosixPath('data/test/cavo/993810.jpg'),\n",
       " PosixPath('data/test/cavo/1158416.jpg'),\n",
       " PosixPath('data/test/cavo/996606.jpg'),\n",
       " PosixPath('data/test/cavo/1158833.jpg'),\n",
       " PosixPath('data/test/cavo/994174.jpg'),\n",
       " PosixPath('data/test/cavo/996043.jpg'),\n",
       " PosixPath('data/test/cavo/997003.jpg'),\n",
       " PosixPath('data/test/cavo/1158743.jpg'),\n",
       " PosixPath('data/test/cavo/993310.jpg'),\n",
       " PosixPath('data/test/cavo/996305.jpg'),\n",
       " PosixPath('data/test/cavo/993787.jpg'),\n",
       " PosixPath('data/test/cavo/12746885.jpg'),\n",
       " PosixPath('data/test/cavo/12746723.jpg'),\n",
       " PosixPath('data/test/cavo/12746481.jpg'),\n",
       " PosixPath('data/test/cavo/12747367.jpg'),\n",
       " PosixPath('data/test/cavo/994140.jpg'),\n",
       " PosixPath('data/test/cavo/1158435.jpg'),\n",
       " PosixPath('data/test/cavo/1158890.jpg'),\n",
       " PosixPath('data/test/cavo/12747278.jpg'),\n",
       " PosixPath('data/test/cavo/1158907.jpg'),\n",
       " PosixPath('data/test/cavo/993779.jpg'),\n",
       " PosixPath('data/test/cavo/992955.jpg'),\n",
       " PosixPath('data/test/cavo/1158812.jpg'),\n",
       " PosixPath('data/test/cavo/1158452.jpg'),\n",
       " PosixPath('data/test/cavo/993754.jpg'),\n",
       " PosixPath('data/test/cavo/992971.jpg'),\n",
       " PosixPath('data/test/cavo/12747060.jpg'),\n",
       " PosixPath('data/test/cavo/1158630.jpg'),\n",
       " PosixPath('data/test/cavo/12747368.jpg'),\n",
       " PosixPath('data/test/cavo/995652.jpg'),\n",
       " PosixPath('data/test/cavo/993901.jpg'),\n",
       " PosixPath('data/test/cavo/996171.jpg'),\n",
       " PosixPath('data/test/cavo/1158794.jpg'),\n",
       " PosixPath('data/test/cavo/993476.jpg'),\n",
       " PosixPath('data/test/cavo/1158499.jpg'),\n",
       " PosixPath('data/test/cavo/996018.jpg'),\n",
       " PosixPath('data/test/cavo/1158757.jpg'),\n",
       " PosixPath('data/test/cavo/12746601.jpg'),\n",
       " PosixPath('data/test/cavo/994009.jpg'),\n",
       " PosixPath('data/test/cavo/1158874.jpg'),\n",
       " PosixPath('data/test/cavo/1158749.jpg'),\n",
       " PosixPath('data/test/cavo/12747093.jpg'),\n",
       " PosixPath('data/test/cavo/993382.jpg'),\n",
       " PosixPath('data/test/cavo/997017.jpg'),\n",
       " PosixPath('data/test/cavo/995195.jpg'),\n",
       " PosixPath('data/test/cavo/12746590.jpg'),\n",
       " PosixPath('data/test/cavo/12746587.jpg'),\n",
       " PosixPath('data/test/cavo/12747115.jpg'),\n",
       " PosixPath('data/test/cavo/993154.jpg'),\n",
       " PosixPath('data/test/cavo/1158758.jpg'),\n",
       " PosixPath('data/test/cavo/12747178.jpg'),\n",
       " PosixPath('data/test/cavo/12747129.jpg'),\n",
       " PosixPath('data/test/cavo/1158772.jpg'),\n",
       " PosixPath('data/test/cavo/993024.jpg'),\n",
       " PosixPath('data/test/cavo/1158820.jpg'),\n",
       " PosixPath('data/test/cavo/1158902.jpg'),\n",
       " PosixPath('data/test/cavo/12747015.jpg'),\n",
       " PosixPath('data/test/cavo/995316.jpg'),\n",
       " PosixPath('data/test/cavo/12746470.jpg'),\n",
       " PosixPath('data/test/cavo/994040.jpg'),\n",
       " PosixPath('data/test/cavo/1158864.jpg'),\n",
       " PosixPath('data/test/cavo/992977.jpg'),\n",
       " PosixPath('data/test/cavo/1158796.jpg'),\n",
       " PosixPath('data/test/cavo/994704.jpg'),\n",
       " PosixPath('data/test/cavo/12747085.jpg'),\n",
       " PosixPath('data/test/cavo/996295.jpg'),\n",
       " PosixPath('data/test/cavo/995331.jpg'),\n",
       " PosixPath('data/test/cavo/992678.jpg'),\n",
       " PosixPath('data/test/cavo/1158771.jpg'),\n",
       " PosixPath('data/test/cavo/993261.jpg'),\n",
       " PosixPath('data/test/cavo/12746966.jpg'),\n",
       " PosixPath('data/test/cavo/12747274.jpg'),\n",
       " PosixPath('data/test/cavo/1158761.jpg'),\n",
       " PosixPath('data/test/cavo/992719.jpg'),\n",
       " PosixPath('data/test/cavo/1158478.jpg'),\n",
       " PosixPath('data/test/dolio/14435641.jpg'),\n",
       " PosixPath('data/test/dolio/14435612.jpg'),\n",
       " PosixPath('data/test/dolio/14435653.jpg'),\n",
       " PosixPath('data/test/dolio/14435695.jpg'),\n",
       " PosixPath('data/test/dolio/14435631.jpg'),\n",
       " PosixPath('data/test/dolio/14435640.jpg'),\n",
       " PosixPath('data/test/dolio/14435602.jpg'),\n",
       " PosixPath('data/test/dolio/14435637.jpg'),\n",
       " PosixPath('data/test/dolio/14435633.jpg'),\n",
       " PosixPath('data/test/dolio/14435659.jpg'),\n",
       " PosixPath('data/test/dolio/14435597.jpg'),\n",
       " PosixPath('data/test/dolio/14435608.jpg'),\n",
       " PosixPath('data/test/dolio/14435643.jpg'),\n",
       " PosixPath('data/test/dolio/14435660.jpg'),\n",
       " PosixPath('data/test/dolio/14435598.jpg'),\n",
       " PosixPath('data/test/dolio/13626049.jpg'),\n",
       " PosixPath('data/test/dolio/14435618.jpg'),\n",
       " PosixPath('data/test/dolio/14435615.jpg'),\n",
       " PosixPath('data/test/dolio/14435625.jpg'),\n",
       " PosixPath('data/test/dolio/14435688.jpg'),\n",
       " PosixPath('data/test/dolio/14435642.jpg'),\n",
       " PosixPath('data/test/dolio/14435667.jpg'),\n",
       " PosixPath('data/test/dolio/14435614.jpg'),\n",
       " PosixPath('data/test/dolio/14435644.jpg'),\n",
       " PosixPath('data/test/dolio/14435627.jpg'),\n",
       " PosixPath('data/test/dolio/14435679.jpg'),\n",
       " PosixPath('data/test/dolio/14435691.jpg'),\n",
       " PosixPath('data/test/dolio/14435621.jpg'),\n",
       " PosixPath('data/test/dolio/14435666.jpg'),\n",
       " PosixPath('data/test/dolio/14435622.jpg'),\n",
       " PosixPath('data/test/dolio/14435676.jpg'),\n",
       " PosixPath('data/test/dolio/14435658.jpg'),\n",
       " PosixPath('data/test/dolio/14435684.jpg'),\n",
       " PosixPath('data/test/dolio/14435664.jpg'),\n",
       " PosixPath('data/test/dolio/14435624.jpg'),\n",
       " PosixPath('data/test/dolio/14435654.jpg'),\n",
       " PosixPath('data/test/dolio/14435682.jpg'),\n",
       " PosixPath('data/test/dolio/14435689.jpg'),\n",
       " PosixPath('data/test/dolio/14435648.jpg'),\n",
       " PosixPath('data/test/dolio/992756.jpg'),\n",
       " PosixPath('data/test/dolio/14435681.jpg'),\n",
       " PosixPath('data/test/dolio/14435674.jpg'),\n",
       " PosixPath('data/test/dolio/14435687.jpg'),\n",
       " PosixPath('data/test/dolio/14435634.jpg'),\n",
       " PosixPath('data/test/dolio/14435606.jpg'),\n",
       " PosixPath('data/test/dolio/14435609.jpg'),\n",
       " PosixPath('data/test/dolio/14435675.jpg'),\n",
       " PosixPath('data/test/dolio/14435603.jpg'),\n",
       " PosixPath('data/test/dolio/14435617.jpg'),\n",
       " PosixPath('data/test/dolio/14435610.jpg'),\n",
       " PosixPath('data/test/dolio/14435629.jpg'),\n",
       " PosixPath('data/test/dolio/14435694.jpg'),\n",
       " PosixPath('data/test/dolio/14435662.jpg'),\n",
       " PosixPath('data/test/dolio/14435632.jpg'),\n",
       " PosixPath('data/test/dolio/14435596.jpg'),\n",
       " PosixPath('data/test/dolio/14435663.jpg'),\n",
       " PosixPath('data/test/dolio/14435626.jpg'),\n",
       " PosixPath('data/test/dolio/14435630.jpg'),\n",
       " PosixPath('data/test/dolio/14435670.jpg'),\n",
       " PosixPath('data/test/dolio/14435692.jpg'),\n",
       " PosixPath('data/test/dolio/14435600.jpg'),\n",
       " PosixPath('data/test/dolio/14435657.jpg'),\n",
       " PosixPath('data/test/dolio/14435690.jpg'),\n",
       " PosixPath('data/test/dolio/14435683.jpg'),\n",
       " PosixPath('data/test/dolio/995737.jpg'),\n",
       " PosixPath('data/test/dolio/14435680.jpg'),\n",
       " PosixPath('data/test/dolio/14435619.jpg'),\n",
       " PosixPath('data/test/dolio/14435651.jpg'),\n",
       " PosixPath('data/test/dolio/14435628.jpg'),\n",
       " PosixPath('data/test/dolio/14435616.jpg'),\n",
       " PosixPath('data/test/dolio/14435605.jpg'),\n",
       " PosixPath('data/test/dolio/14435636.jpg'),\n",
       " PosixPath('data/test/dolio/14435604.jpg'),\n",
       " PosixPath('data/test/dolio/14435678.jpg'),\n",
       " PosixPath('data/test/dolio/14435652.jpg'),\n",
       " PosixPath('data/test/dolio/14435685.jpg'),\n",
       " PosixPath('data/test/dolio/14435668.jpg'),\n",
       " PosixPath('data/test/dolio/14435601.jpg'),\n",
       " PosixPath('data/test/dolio/14435635.jpg'),\n",
       " PosixPath('data/test/dolio/14435669.jpg'),\n",
       " PosixPath('data/test/dolio/14435661.jpg'),\n",
       " PosixPath('data/test/dolio/14435611.jpg'),\n",
       " PosixPath('data/test/dolio/14435655.jpg'),\n",
       " PosixPath('data/test/dolio/14435647.jpg'),\n",
       " PosixPath('data/test/dolio/14435650.jpg'),\n",
       " PosixPath('data/test/dolio/14435677.jpg'),\n",
       " PosixPath('data/test/dolio/14435649.jpg'),\n",
       " PosixPath('data/test/dolio/14435607.jpg'),\n",
       " PosixPath('data/test/dolio/14435613.jpg'),\n",
       " PosixPath('data/test/dolio/14435646.jpg'),\n",
       " PosixPath('data/test/dolio/14435638.jpg'),\n",
       " PosixPath('data/test/dolio/14435673.jpg'),\n",
       " PosixPath('data/test/dolio/14435623.jpg'),\n",
       " PosixPath('data/test/dolio/14435665.jpg'),\n",
       " PosixPath('data/test/dolio/14435656.jpg'),\n",
       " PosixPath('data/test/dolio/14435671.jpg'),\n",
       " PosixPath('data/test/dolio/14435599.jpg'),\n",
       " PosixPath('data/test/dolio/14435645.jpg'),\n",
       " PosixPath('data/test/dolio/14435620.jpg'),\n",
       " PosixPath('data/test/dolio/14435672.jpg'),\n",
       " PosixPath('data/test/onca/14431807.jpg'),\n",
       " PosixPath('data/test/onca/14431775.jpg'),\n",
       " PosixPath('data/test/onca/14431751.jpg'),\n",
       " PosixPath('data/test/onca/14431826.jpg'),\n",
       " PosixPath('data/test/onca/14431812.jpg'),\n",
       " PosixPath('data/test/onca/14431821.jpg'),\n",
       " PosixPath('data/test/onca/14431771.jpg'),\n",
       " PosixPath('data/test/onca/994458.jpg'),\n",
       " PosixPath('data/test/onca/14431748.jpg'),\n",
       " PosixPath('data/test/onca/14431758.jpg'),\n",
       " PosixPath('data/test/onca/14431825.jpg'),\n",
       " PosixPath('data/test/onca/14431844.jpg'),\n",
       " PosixPath('data/test/onca/14431769.jpg'),\n",
       " PosixPath('data/test/onca/14431833.jpg'),\n",
       " PosixPath('data/test/onca/14431810.jpg'),\n",
       " PosixPath('data/test/onca/14431832.jpg'),\n",
       " PosixPath('data/test/onca/14431827.jpg'),\n",
       " PosixPath('data/test/onca/14431746.jpg'),\n",
       " PosixPath('data/test/onca/14431779.jpg'),\n",
       " PosixPath('data/test/onca/14431813.jpg'),\n",
       " PosixPath('data/test/onca/14431803.jpg'),\n",
       " PosixPath('data/test/onca/14431800.jpg'),\n",
       " PosixPath('data/test/onca/14431787.jpg'),\n",
       " PosixPath('data/test/onca/14431781.jpg'),\n",
       " PosixPath('data/test/onca/14431754.jpg'),\n",
       " PosixPath('data/test/onca/14431792.jpg'),\n",
       " PosixPath('data/test/onca/14431799.jpg'),\n",
       " PosixPath('data/test/onca/14431809.jpg'),\n",
       " PosixPath('data/test/onca/14431829.jpg'),\n",
       " PosixPath('data/test/onca/14431822.jpg'),\n",
       " PosixPath('data/test/onca/14431804.jpg'),\n",
       " PosixPath('data/test/onca/14431761.jpg'),\n",
       " PosixPath('data/test/onca/14431798.jpg'),\n",
       " PosixPath('data/test/onca/14431806.jpg'),\n",
       " PosixPath('data/test/onca/14431841.jpg'),\n",
       " PosixPath('data/test/onca/14431797.jpg'),\n",
       " PosixPath('data/test/onca/14431786.jpg'),\n",
       " PosixPath('data/test/onca/14431745.jpg'),\n",
       " PosixPath('data/test/onca/14431815.jpg'),\n",
       " PosixPath('data/test/onca/14431764.jpg'),\n",
       " PosixPath('data/test/onca/14431784.jpg'),\n",
       " PosixPath('data/test/onca/14431796.jpg'),\n",
       " PosixPath('data/test/onca/14431824.jpg'),\n",
       " PosixPath('data/test/onca/14431743.jpg'),\n",
       " PosixPath('data/test/onca/14431757.jpg'),\n",
       " PosixPath('data/test/onca/14431836.jpg'),\n",
       " PosixPath('data/test/onca/14431837.jpg'),\n",
       " PosixPath('data/test/onca/14431817.jpg'),\n",
       " PosixPath('data/test/onca/14431768.jpg'),\n",
       " PosixPath('data/test/onca/14431814.jpg'),\n",
       " PosixPath('data/test/onca/14431835.jpg'),\n",
       " PosixPath('data/test/onca/14431839.jpg'),\n",
       " PosixPath('data/test/onca/14431816.jpg'),\n",
       " PosixPath('data/test/onca/14431762.jpg'),\n",
       " PosixPath('data/test/onca/14431766.jpg'),\n",
       " PosixPath('data/test/onca/14431749.jpg'),\n",
       " PosixPath('data/test/onca/14431770.jpg'),\n",
       " PosixPath('data/test/onca/14431783.jpg'),\n",
       " PosixPath('data/test/onca/14431760.jpg'),\n",
       " PosixPath('data/test/onca/14431819.jpg'),\n",
       " PosixPath('data/test/onca/14431840.jpg'),\n",
       " PosixPath('data/test/onca/14431767.jpg'),\n",
       " PosixPath('data/test/onca/14431753.jpg'),\n",
       " PosixPath('data/test/onca/14431759.jpg'),\n",
       " PosixPath('data/test/onca/14431777.jpg'),\n",
       " PosixPath('data/test/onca/14431802.jpg'),\n",
       " PosixPath('data/test/onca/14431801.jpg'),\n",
       " PosixPath('data/test/onca/14431778.jpg'),\n",
       " PosixPath('data/test/onca/14431788.jpg'),\n",
       " PosixPath('data/test/onca/14431793.jpg'),\n",
       " PosixPath('data/test/onca/14431774.jpg'),\n",
       " PosixPath('data/test/onca/14431752.jpg'),\n",
       " PosixPath('data/test/onca/14431780.jpg'),\n",
       " PosixPath('data/test/onca/14431756.jpg'),\n",
       " PosixPath('data/test/onca/14431765.jpg'),\n",
       " PosixPath('data/test/onca/14431747.jpg'),\n",
       " PosixPath('data/test/onca/14431830.jpg'),\n",
       " PosixPath('data/test/onca/14431842.jpg'),\n",
       " PosixPath('data/test/onca/14431791.jpg'),\n",
       " PosixPath('data/test/onca/14431794.jpg'),\n",
       " PosixPath('data/test/onca/14431763.jpg'),\n",
       " PosixPath('data/test/onca/14431782.jpg'),\n",
       " PosixPath('data/test/onca/14431818.jpg'),\n",
       " PosixPath('data/test/onca/14431811.jpg'),\n",
       " PosixPath('data/test/onca/14431773.jpg'),\n",
       " PosixPath('data/test/onca/14431772.jpg'),\n",
       " PosixPath('data/test/onca/14431820.jpg'),\n",
       " PosixPath('data/test/onca/14431785.jpg'),\n",
       " PosixPath('data/test/onca/995314.jpg'),\n",
       " PosixPath('data/test/onca/14431750.jpg'),\n",
       " PosixPath('data/test/onca/14431808.jpg'),\n",
       " PosixPath('data/test/onca/14431828.jpg'),\n",
       " PosixPath('data/test/onca/14431744.jpg'),\n",
       " PosixPath('data/test/onca/14431823.jpg'),\n",
       " PosixPath('data/test/onca/14431838.jpg'),\n",
       " PosixPath('data/test/onca/14431795.jpg'),\n",
       " PosixPath('data/test/onca/14431776.jpg'),\n",
       " PosixPath('data/test/onca/14431790.jpg'),\n",
       " PosixPath('data/test/onca/14431834.jpg'),\n",
       " PosixPath('data/test/onca/13107988.jpg')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Path('./data/test')\n",
    "end = '.jpg'\n",
    "list(p.glob('**/*'+end))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
