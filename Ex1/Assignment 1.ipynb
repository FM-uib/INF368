{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with class imbalance:\n",
    "- resampling techniques\n",
    "    - under or over sampling random vs informed\n",
    "    - SMOTE synthetic minor ...\n",
    "- kappa statistics/ MCC Metric\n",
    "- multiclass mcc \"comparing two k-category assignments by a k-category correlation coeeficient\"\n",
    "\n",
    "\n",
    "spatial pyramid pooling in deep convolutional networks for visual recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from pathlib import Path\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import Sequence\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.util import pad\n",
    "from skimage.util import crop\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_Gen(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels, batch_size, shuffle, train):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_labels = len(np.unique(labels))\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.train = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "    \n",
    "    def crop_or_pad(self, image, dim, filename):\n",
    "        x, y, _ = image.shape\n",
    "        if (y < dim and x < dim):\n",
    "            image = pad(image, ((math.ceil((dim - image.shape[0])/2),math.floor((dim - image.shape[0])/2)),\n",
    "                                (math.ceil((dim - image.shape[1])/2),math.floor((dim - image.shape[1])/2)), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "        elif (y >= dim and x >= dim):\n",
    "            if x == 299:\n",
    "                rand1 = 0\n",
    "            else:\n",
    "                rand1 = random.randint(1,x-dim)\n",
    "            if y == 299:\n",
    "                rand2=0\n",
    "            else:\n",
    "                rand2 = random.randint(1,y-dim)\n",
    "            image = crop(image, ((rand1,x-dim-rand1),(rand2,y-dim-rand2),(0,0)))\n",
    "        elif (x >= dim and y < dim):\n",
    "            image = pad(image, ((0,0),\n",
    "                                (math.ceil((dim - y)/2),math.floor((dim - y)/2)), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "            if x != 299:\n",
    "                rand1 = random.randint(1,x-dim)\n",
    "                image = crop(image, ((rand1,x-dim-rand1),\n",
    "                                (0,0), (0,0)))\n",
    "        else:\n",
    "            #if y < dim: print(filename)\n",
    "            image = pad(image, ((math.ceil((dim - x)/2),math.floor((dim - x)/2)),\n",
    "                                (0,0), \n",
    "                                (0,0)), 'constant', constant_values = 255)\n",
    "            if y != 299:\n",
    "                rand2 = random.randint(1,y-dim)\n",
    "                image = crop(image, ((0,0),\n",
    "                                     (rand2,y-dim-rand2), (0,0)))\n",
    "        return image\n",
    "    \n",
    "    def read_im(self, filename, dim):\n",
    "        image = imread(filename)\n",
    "        #image = resize(image, (dim,dim), anti_aliasing = True, mode = \"reflect\")\n",
    "        image = skimage.color.gray2rgb(image)\n",
    "        image = self.crop_or_pad(image, dim, filename)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        image = [self.read_im(filename, 299) for filename in batch_x]\n",
    "        image = (image-np.amin(image))/(np.amax(image)-np.amin(image))\n",
    "        if self.train:    \n",
    "            batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            return np.array(image), np.array(batch_y)\n",
    "        else:\n",
    "            return np.array(image)\n",
    "        #batch_y = keras.utils.to_categorical(batch_y, self.num_labels)\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            fnames_and_labels = list(zip(self.image_filenames, self.labels))\n",
    "            random.shuffle(fnames_and_labels)\n",
    "            self.image_filenames, self.labels = zip(*fnames_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = './data/test/'\n",
    "def fetch_data_set(path, ftype = 'jpg'):\n",
    "    p = Path(path)\n",
    "    files = list(p.glob('**/*.'+ftype))\n",
    "    classes = str(files).split('/')\n",
    "    classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "    classnames, indices = np.unique(classes, return_inverse=True)\n",
    "    dict_classes = dict(zip(classnames, list(range(0,len(classes)))))\n",
    "    return files, classes, dict_classes\n",
    "\n",
    "#np.array([dict_[i] for i in classes])\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from itertools import chain\n",
    "\n",
    "def over_under_sample(files, classes, dict_classes, num_to_undersample = 15000, num_to_oversample = 7500):\n",
    "    files_array = np.array(files).reshape(-1,1)\n",
    "    dic = {}\n",
    "    for i in list(classes):\n",
    "        dic[i] = dic.get(i,0) + 1\n",
    "\n",
    "    classes_to_oversample = dict((k, v) for k, v in dic.items() if v < num_to_oversample)\n",
    "    classes_to_undersample = dict((k, v) for k, v in dic.items() if v > num_to_undersample)    \n",
    "    \n",
    "    for key, value in classes_to_undersample.items():\n",
    "        classes_to_undersample[key] = num_to_undersample\n",
    "    for key, value in classes_to_oversample.items():\n",
    "        classes_to_oversample[key] = num_to_oversample\n",
    "    \n",
    "    ros = RandomOverSampler(sampling_strategy = classes_to_oversample)\n",
    "    rus = RandomUnderSampler(sampling_strategy = classes_to_undersample)\n",
    "    x_over, y_over = ros.fit_resample(files_array, classes)\n",
    "    x_under, y_under = rus.fit_resample(x_over, y_over)\n",
    "    x_under = list(chain(*x_under.tolist()))\n",
    "    return x_under, y_under\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_train_validation_test(data_X, data_Y, test_percent, validation_percent, seed): \n",
    "    assert (test_percent < 1) and (0 < validation_percent) and (validation_percent < 1)\n",
    "    X_tmp, X_val, Y_tmp, Y_val = train_test_split(data_X, data_Y, test_size=validation_percent, shuffle=True, random_state=seed)\n",
    "    \n",
    "    if test_percent != 0:\n",
    "        relative_test_percent = test_percent / (1 - validation_percent)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_tmp, Y_tmp, test_size=relative_test_percent, shuffle=True, random_state=seed)\n",
    "        split_data = [X_train, Y_train, X_val, Y_val, X_test, Y_test]\n",
    "    else:\n",
    "        X_train, Y_train = X_tmp, Y_tmp\n",
    "        split_data = [X_train, Y_train, X_val, Y_val]\n",
    "\n",
    "    return split_data  \n",
    "\n",
    "def encode_labels(labels, OneHot=True, encoder = None):\n",
    "    if OneHot and encoder == None:\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        enc.fit(np.array(labels).reshape(-1, 1))\n",
    "        OneHot = enc.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
    "        return OneHot, enc\n",
    "    else:\n",
    "        OneHot = encoder.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
    "        return OneHot\n",
    "    # mlb \n",
    "    #1 convert labels to multilabel using hardcoded dict\n",
    "    #2 fit mlb\n",
    "    #3 transform \n",
    "    # return labels and encoder\n",
    "    # repeat for when encoder is present\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "files, classes, dict_classes = fetch_data_set(\"./data/imgs/\")\n",
    "# split dataset\n",
    "split = split_data_train_validation_test(files, classes, 0.05, .25, random.randint(1,10000))\n",
    "split[0], split[1] = over_under_sample(split[0], split[1], dict_classes, num_to_undersample = 3000, num_to_oversample = 3000)\n",
    "# load label encoder and transform labels\n",
    "split[1], OH_enc = encode_labels(split[1], OneHot = True)\n",
    "split[3] = encode_labels(split[3], encoder = OH_enc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from collections import Counter\n",
    "#print(sorted(Counter(classes).items()))\n",
    "#print(sorted(Counter(y_re).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c,_= np.unique(split[5], return_inverse=True)\n",
    "#len(c)\n",
    "#b = {}\n",
    "#for i in list(split[5]):\n",
    "#    b[i] = b.get(i,0) + 1\n",
    "\n",
    "#counts = dict((k, v) for k, v in b.items() if v > 1)\n",
    "#counts\n",
    "\n",
    "#f, c, d = fetch_data_set('./data/imgs')\n",
    "#d['cavo'] = [0,1,1]\n",
    "#[d[i] for i in c]\n",
    "d_ = d\n",
    "d_['Annelida'] = ['Annelida', 'Metazoa', 'Eukaryota']\n",
    "d_['Bivalvia__Molusca'] = ['Bivalvia__Molusca', 'Mollusca', 'Metazoa', 'Eukaryota']\n",
    "d_['Brachyura'] = ['Brachyura', 'Decapoda', 'Malacostraca', 'Arthropoda', 'Metazoa', 'Eukaryota']\n",
    "d_['Candaciidae'] = ['Candaciidae', 'Calanoida', 'Copepoda', 'Maxillopoda', 'Arthropoda', 'Metazoa', 'Eukaryota']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./data/test/') \n",
    "files = list(p.glob('**/*.jpg'))\n",
    "classes = str(files).split('/')\n",
    "classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "classnames, indices = np.unique(classes, return_inverse=True)\n",
    "labels = keras.utils.to_categorical(indices, len(np.unique(indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./data/imgs/') \n",
    "files = list(p.glob('**/*.jpg'))\n",
    "classes = str(files).split('/')\n",
    "classes = [classes[i] for i in list(range(2,len(classes),3)) ]\n",
    "classnames, indices = np.unique(classes, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(list(counts.values()))\n",
    "counts\n",
    "classes_to_oversample = dict((k, v) for k, v in b.items() if v < 7500)\n",
    "classes_to_undersample = dict((k, v) for k, v in b.items() if v > 15000)\n",
    "#assign new target values for sampling\n",
    "for key, value in classes_to_undersample.items():\n",
    "    classes_to_undersample[key] = 15000\n",
    "classes_to_undersample.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "d = {0: 150, 1: 300}\n",
    "ros = RandomOverSampler(random_state = 0, sampling_strategy = d)\n",
    "x_re, y_re = ros.fit_resample(np.array(split[0]).reshape(-1,1), split[1])\n",
    "from collections import Counter\n",
    "print(sorted(Counter(split[1]).items()))\n",
    "print(sorted(Counter(y_re).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "x_re = list(chain(*x_re.tolist()))\n",
    "y_re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras_metrics\n",
    "#metrics=[keras_metrics.precision(), keras_metrics.recall()\n",
    "b = {}\n",
    "for i in list(np.array(x_re)):\n",
    "    b[i] = b.get(i,0) + 1\n",
    "\n",
    "dict((k, v) for k, v in b.items() if v > 1)\n",
    "\n",
    "#x_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import keras_metrics\n",
    "from keras import backend as K\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(40, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "#num_training_samples=len(files)\n",
    "\n",
    "files, classes, dict_classes =  fetch_data_set('./data/test')\n",
    "split = split_data_train_validation_test(files, classes, .1, .25, 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(np.array([dict_classes[i] for i in split[5]]), (test_pred), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "#len(split[5]), len(test_pred)\n",
    "#len(np.argmax(model_pred, axis = 1, out = None)), len(test_pred), len(split[5])\n",
    "#[dict_classes[i] for i in split[5]], list(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dict_classes[i] for i in split[1]]\n",
    "import pandas as pd\n",
    "model_pred = model.predict_generator(generator = test_batch, steps = (len(split[4]) // batch_size))\n",
    "#test_actu = np.argmax([dict_classes[i] for i in split[5]], axis = 1, out = None)\n",
    "test_pred = np.argmax(model_pred, axis = 1, out = None)\n",
    "pd.crosstab(split[5], test_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 267/4000 [=>............................] - ETA: 27:53 - loss: 15.7199 - precision: 0.0000e+00 - recall: 0.0000e+00 - acc: 0.0215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-28:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 355, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 354, in put\n",
      "    with self._wlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-2-0e3df50a5473>\", line 59, in __getitem__\n",
      "    image = [self.read_im(filename, 299) for filename in batch_x]\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-2-0e3df50a5473>\", line 59, in <listcomp>\n",
      "    image = [self.read_im(filename, 299) for filename in batch_x]\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-2-0e3df50a5473>\", line 59, in __getitem__\n",
      "    image = [self.read_im(filename, 299) for filename in batch_x]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"<ipython-input-2-0e3df50a5473>\", line 59, in <listcomp>\n",
      "    image = [self.read_im(filename, 299) for filename in batch_x]\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-2-0e3df50a5473>\", line 50, in read_im\n",
      "    image = imread(filename)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-2-0e3df50a5473>\", line 50, in read_im\n",
      "    image = imread(filename)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/_io.py\", line 62, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/_io.py\", line 62, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/manage_plugins.py\", line 214, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/manage_plugins.py\", line 214, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/_plugins/pil_plugin.py\", line 39, in imread\n",
      "    im = Image.open(fname)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/skimage/io/_plugins/pil_plugin.py\", line 39, in imread\n",
      "    im = Image.open(fname)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 349, in put\n",
      "    obj = ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 50, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c4f8f5b20cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     max_queue_size=32)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "def step_decay_schedule(base_lr=1e-4, decay_factor=0.5, step_decay=5):\n",
    "    def schedule(epoch):\n",
    "        ## Multiply learning rate by 'decay_factor' every 'step_size' epochs (note that epoch is indexed from 0):\n",
    "        updated_lr = base_lr * (decay_factor ** np.floor((epoch + 1) / step_decay))  \n",
    "        return updated_lr    \n",
    "    return LearningRateScheduler(schedule)\n",
    "\n",
    "batch_size = 30\n",
    "training_batch = MY_Gen(split[0], split[1], batch_size, shuffle = True, train = True)\n",
    "validation_batch = MY_Gen(split[2], split[3], batch_size, shuffle = True, train = True)\n",
    "test_batch = MY_Gen(split[4], split[5], batch_size, shuffle = False, train = False)\n",
    "\n",
    "lr_policy = step_decay_schedule(base_lr=0.01, decay_factor=0.75, step_decay=10)\n",
    "callback_list = [lr_policy]\n",
    "callback_list.append(EarlyStopping(monitor='val_loss',\n",
    "                                   min_delta=0,\n",
    "                                   patience=0,\n",
    "                                   verbose=0,\n",
    "                                   mode='auto',\n",
    "                                   baseline=None,\n",
    "                                   restore_best_weights=False))\n",
    "callback_list.append(ModelCheckpoint(\"./checkpoints/model_{epoch:02d}.hdf5\", \n",
    "                                              monitor='val_loss', \n",
    "                                              verbose=0, \n",
    "                                              save_best_only=True, save_weights_only=False))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=[keras_metrics.precision(), keras_metrics.recall(),'accuracy'])\n",
    "\n",
    "history = model.fit_generator(generator=training_batch,\n",
    "                    validation_data = validation_batch,\n",
    "                    validation_steps = (len(split[2]) // batch_size),\n",
    "                    steps_per_epoch=(len(split[0]) // batch_size),\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    callbacks = callback_list,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=16,\n",
    "                    max_queue_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_cm_calib(model, data, labels, OH_enc, dict_classes, batch_size = 100):\n",
    "    import pandas as pd\n",
    "    #split[5] = encode_labels(split[5], OneHot=True, encoder = OH_enc)\n",
    "    test_batch = MY_Gen(data, labels, batch_size = batch_size, shuffle = False, train = False)\n",
    "    model_pred = model.predict_generator(generator = test_batch, steps = (len(data) // batch_size))\n",
    "    t = {v: k for k, v in dict_classes.items()}\n",
    "#test_actu = OH_enc.inverse_transform(split[5]).flatten()\n",
    "    test_pred = [t[i] for i in np.argmax(model_pred, axis = 1, out = None)]\n",
    "    cm = pd.crosstab(np.array(labels), np.array(test_pred), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    bins = np.arange(0,1.1,0.1)\n",
    "    acc = np.zeros([10,1])\n",
    "    pred = model_pred\n",
    "    actu = np.array(test_pred) == np.array(labels)\n",
    "    actu = actu.reshape([len(data),1])\n",
    "    for i in range(len(bins)-1):\n",
    "        pred = pred*(OH_enc.transform(np.array(labels).reshape(-1,1)).toarray() == 1)\n",
    "        tmp = ((pred > bins[i]) & (pred < bins[i+1]))\n",
    "        if sum(tmp.flatten()) == 0:\n",
    "            acc[i] = 0\n",
    "        else:\n",
    "            acc[i] = np.sum(tmp)/np.sum(pred > 0)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(np.arange(0,10,1), height = acc.flatten())\n",
    "    plt.xticks(np.arange(0,10,1), ['0-0.1','0.1-0.2','0.2-0.3','0.3-0.4','0.4-0.5','0.5-0.6','0.6-0.7','0.7-0.8','0.8-0.9','0.9-1'])\n",
    "    plt.xlabel(\"Softmax Intervals\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Sofmax calibration\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(pred.flatten(), range = (0,1))\n",
    "    plt.xlabel(\"Softmax Intervals\")\n",
    "    plt.ylabel(\"No of Samples\")\n",
    "    plt.title(\"Sofmax calibration\")\n",
    "    plt.show()    \n",
    "    return cm\n",
    "plot_cm_calib(model, split[2], split[3], OH_enc, dict_classes, batch_size = 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier probably misclassifies some images. Investigate how well the accuracy of the\n",
    "classifications match the softmax output values, for instance as a histogram with softmax\n",
    "intervals (buckets) on the x-axis (e.g. 0.70-0.75), and the percentage correct classifications for\n",
    "each bucket (e.g. 68%) on the y-axis. Is your classifier overconfident or underconfident, or\n",
    "neutral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "with open(\"data/output/models/pretra_INC.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "model.save_weights(\"data/output/models/pretra_INC_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im=np.asarray(Image.open(files[1]).resize([299,299]))\n",
    "#im = im/np.amax(im)\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(image[7])\n",
    "#plt.show()\n",
    "#ind = np.arange(105)\n",
    "#isinstance(classes, list)\n",
    "#len(classes)\n",
    "import random\n",
    "c=list(zip(files,classes))\n",
    "random.shuffle(c)\n",
    "files,classes = zip (*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "im = resize(imread(files[1]), (100, 100))\n",
    "im = skimage.color.gray2rgb(im)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im = np.array([(Image.open(file_name).resize([299,299])) for file_name in files])\n",
    "#im2 = np.array([\n",
    "           # resize(imread(file_name), (299, 299))\n",
    "            #   for file_name in files])\n",
    "#len(im2)\n",
    "#im2.shape\n",
    "#im.shape\n",
    "def norm_im(filename, dim):\n",
    "    image = imread(filename)\n",
    "    image = resize(image, (dim,dim), mode = \"edge\")\n",
    "    image = (image-np.amin(image))/(np.amax(image)-np.amin(image))\n",
    "    return image\n",
    "image = np.array([norm_im(filename, 100) for filename in files])\n",
    "\n",
    "\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files[range(1,10)]\n",
    "list(indices)\n",
    "keras.utils.to_categorical(indices, 5)\n",
    "len(np.unique(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "num_training_samples=len(files)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "my_training_batch_generator = MY_Gen(files, labels, batch_size)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(generator=my_training_batch_generator,\n",
    "                                          steps_per_epoch=(num_training_samples // batch_size),\n",
    "                                          epochs=10,\n",
    "                                          verbose=1,\n",
    "                                          use_multiprocessing=False,\n",
    "                                          workers=16,\n",
    "                                          max_queue_size=32,\n",
    "                             shuffle = True)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path('./data/imgs/') \n",
    "classes = [x for x in p.iterdir() if x.is_dir()]\n",
    "files = list(p.glob('**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels = [\n",
    "    (\"legs\",\"blue\", \"jeans\"),\n",
    "    (\"no_legs\",\"blue\", \"dress\"),\n",
    "    (\"no_legs\",\"red\", \"dress\"),\n",
    "    (\"no_legs\",\"red\", \"shirt\"),\n",
    "    (\"no_legs\",\"blue\", \"shirt\"),\n",
    "    (\"no_legs\",\"black\", \"jeans\")]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "\n",
    "mlb.classes_\n",
    "\n",
    "mlb.transform([(\"blue\", \"dress\", \"no_legs\")]), mlb.inverse_transform(np.array([[0, 1, 1, 0, 0, 1, 0, 0], [1, 1, 1, 0, 0, 1, 0, 0]]))\n",
    "#np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.array(split[5]).reshape(-1, 1))\n",
    "OneHot=enc.transform(np.array(split[5]).reshape(-1, 1)).toarray()\n",
    "#enc.inverse_transform(OneHot)\n",
    "OneHot[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./data/test')\n",
    "end = '.jpg'\n",
    "list(p.glob('**/*'+end))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
