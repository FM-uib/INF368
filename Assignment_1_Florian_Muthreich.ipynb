{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Florian Muthreich   ---   INF368   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next I download the MNIST dataset. It is already split in test and training set and saved to variables. The images are stored separately from the labels in arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The MNIST set has been downloaded and I can show the dimensions of the dataset.\n",
    "In total there are 70000 images of handwritten numbers. 60000 in the train set and 10000 in the test set. each image is 28 x 28 pixels and has only one channel, which means each cell indicates the intensity of a pixel. The labels are stored separately in their own array, which basically has the shape of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\",x_train.shape, \", labels\", y_train.shape)\n",
    "print(\"Test set:\",x_test.shape, \", labels\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here I plot 6 random images from the MNIST dataset. 3 from the train and 3 from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (14,10)\n",
    "for x in range(6):\n",
    "    if x<3:\n",
    "        i = random.randint(1,x_train.shape[0])\n",
    "        plt.subplot(2, 3, x+1)\n",
    "        plt.imshow(x_tr[i])\n",
    "        plt.title(\"Train img: {}; label: {}\".format(i,y_train[i]))\n",
    "    else:\n",
    "        i = random.randint(1,x_test.shape[0])\n",
    "        plt.subplot(2, 3, x+1)\n",
    "        plt.imshow(x_te[i])\n",
    "        plt.title(\"Test img: {}; label: {}\".format(i,y_test[i]))\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I defined a function to transform the training data into 4D vectors. The same function also converts the labels into one hot coded vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_rec(data, labels, channels):\n",
    "    tmp = data.reshape(data.shape[0], channels, data.shape[1], data.shape[2])\n",
    "    tmp = tmp.astype(\"float32\")\n",
    "    tmp = tmp/np.amax(x_train)\n",
    "    lbl = keras.utils.to_categorical(labels, len(np.unique(labels)))\n",
    "    return tmp, lbl\n",
    "\n",
    "x_train, y_train = res_rec(x_train, y_train, 1)\n",
    "x_test, y_test = res_rec(x_test, y_test, 1)\n",
    "\n",
    "print(\"Training set:\",x_train.shape, \", labels\", y_train.shape)\n",
    "print(\"Test set:\",x_test.shape, \", labels\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This model has 1 hidden layer with 50 units and a second layer with 10 softmax units. I added another layer before the first hidden layer to convert the picture data from a 28 x 28 matrix into a vector, this is done by the flatten layer. there are no weights learned in this layer, hust a transformation from 2 dimensions to 1. The number of parameters can be easily derived from this model description.\n",
    "\n",
    "We have 784 input features that are connected to 50 hidden units in the first layer (1). Each unit also has a weight for the bias (2). In the next layer, the 50 units of the first hidden unit are connected to the 10 Softmax units with weights for each connection in addition to weights for the bias of each node in the Softmax layer (3). This brings the total to 39760 parameters. We can see the number of trainable parameters with the model.summary() command.\n",
    "\n",
    "1) 784 * 50 = 39200\n",
    "\n",
    "2) 39200 + 50 = 39250\n",
    "\n",
    "3) 50 * 10 + 10 = 510\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (1,28, 28), name = \"flatten\"),\n",
    "    Dense(50, name = \"hidden_1\"),\n",
    "    Activation(\"sigmoid\", name = \"act_hidden_1\"),\n",
    "    Dense(10, name = \"out\"),\n",
    "    Activation(\"softmax\", name = \"act_out\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer = \"sgd\",\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The training data is split into training and validation (development) set. I randomly select 5000 (1/12) images of the training set and their corresponding labels and save them as the validation set, to check performance during training. \n",
    "\n",
    "In the last line of the code I call a function that will save model checkpoints and the weights after each epoch. This way I can go back to each point during training and recreate the model and its weight from that point in the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = random.sample(range(x_train.shape[0]), int(1/12 * x_train.shape[0]))\n",
    "x_valid, y_valid = x_train[ind], y_train[ind]\n",
    "x_train, y_train = np.delete(x_train, ind, axis = 0), np.delete(y_train, ind, axis = 0)\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\"./checkpoints/model_{epoch:02d}.hdf5\", \n",
    "                                              monitor='val_loss', \n",
    "                                              verbose=0, \n",
    "                                              save_best_only=False, save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = model.fit(x_train, y_train, \n",
    "                    epochs = 5, batch_size = 64, \n",
    "                    callbacks = [checkpoints], \n",
    "                    validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tested_eval = model.evaluate(x_test, y_test, batch_size = 128)\n",
    "tested_pred = model.predict(x_test, batch_size = 128)\n",
    "\n",
    "actu = np.argmax(y_test, axis = 1, out = None)\n",
    "pred = np.argmax(tested_pred, axis = 1, out = None)\n",
    "\n",
    "confusion = pd.crosstab(actu, pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,5)\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(range(1, 6), trained.history[\"acc\"], color = \"blue\")\n",
    "plt.plot(range(1, 6), trained.history[\"val_acc\"], color = \"green\")\n",
    "plt.plot(5, tested_eval[1], marker = \"o\", color = \"orange\")\n",
    "plt.ylim(0.5, 1), plt.xticks(range(1,6))\n",
    "plt.legend([\"accuracy train\", \"accuracy valid\", \"accuracy test\"])\n",
    "plt.title(\"Accuracy MLP model\"), plt.xlabel(\"Epochs\"), plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 6), trained.history[\"loss\"], color = \"blue\")\n",
    "plt.plot(range(1, 6), trained.history[\"val_loss\"], color = \"green\")\n",
    "plt.plot(5, tested_eval[0], marker = \"o\", color = \"orange\")\n",
    "plt.ylim(0), plt.xticks(range(1,6))\n",
    "plt.legend([\"loss train\", \"loss valid\", \"loss test\"])\n",
    "plt.title(\"Loss MLP model\"), plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Saving the model to json string and to a file. \n",
    "\n",
    "Saving the weights of the latest model to hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "with open(\"model_mlp.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "model.save_weights(\"weights_mlp.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from json and reload the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_mlp.json\", \"r\") as json_file:\n",
    "    load_json_model = json_file.read()\n",
    "    \n",
    "loaded_model = keras.models.model_from_json(load_json_model)\n",
    "loaded_model.load_weights(\"weights_mlp.hdf5\", by_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet will load the model after the first training epoch and restore the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(\"checkpoints/model_01.hdf5\")\n",
    "new_model.summary()\n",
    "new_model.evaluate(x_train, y_train, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbcallback = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                            histogram_freq=0, \n",
    "                            batch_size=32, \n",
    "                            write_graph=True, write_grads=True, write_images=True, \n",
    "                            embeddings_freq=0, embeddings_layer_names=None, \n",
    "                            embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "callbacks = [tbcallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
